---
title: "A Vision for Wet Lab Digitalization"
description: "My vision and goals for achieving digitalized, reproducible, 
interoperable, and automated life science research."
author: "Hugo Åkerstrand"
date: "2025-11-16"
categories:
  - Digitalization of Life Science Research
  - Computational Biology
  - Pipeline Development
  - Reproducible Research
---

## The one thing that I have loved doing this year was working on pipeline development for wet lab data analysis
More specifically, I have been drawing upon my domain knowledge on flow
cytometry to develop an R package for automated data analysis. In case you don't
know what I'm talking about, flow cytometry is an advanced methodology combining
fluidics, optics, and electronics to analyze thousands of cells per second for
characteristics and classification. And that's *really neat*, but it has a big
bottleneck in the subsequent data analysis relying on point-and-click,
proprietary software. Such analysis is inefficient, subjective, and hard to
normalize across different experiments.

### And this problem is bigger than flow cytometry
Life science research has this problem in general: specialized data from
specialized techniques has lead to the development of specialized point-and-click
proprietary software. And that has its place, absolutely — I'm not being
high-nosed saying everyone should *just do* statistical programming instead —
but the truth is that the above workflow has some serious issues and
limitations:

- It suffers from low reproducibility - essentially the user has to go out of their
way to document the settings of every specific analysis.
- Its inefficiency means that it takes up a lot of time and creates resistance to address potential mistakes, do necessary changes (SOP versions, request from collaborators or reviewers), or in general tune the analysis by iterative experimentation.
- Specialized software means separating data analysis and its visualization, making it hard to
match any given graph with a version of the analysis.
- It creates data silos that are hard to combine into a cohesive whole - which 
diminishes the value of the whole body of data to its individual components and 
foregoes their use in ML and LLMs. 

Digitalization - the act of creating code-driven pipelines for data processing 
and visualization - address all of the above. It also frees up time for scientists 
and technicians to think, read, plan, and collaborate. Finally, It enables live monitoring of long experiments, to identify bottlenecks or catch failing
batches early on. 

## How to reach Nirvana (digitalization)?
I boil this down to three specific components:

- Automate intermediate data analysis
- Develop relevant statistical programming packages (e.g. for flow cytometry analysis)
- Centralize data into databases to generate automatic, standardized reports, make it 
available to non-coding stakehodlers, and use it for ML and with LLMs 

Luckily, we can tap into a lf a lot of the pre-existing infrastructure — R and
Python packages and tools that have already been developed for other data science
tasks.

### My (current) vision for lab digitalization

Here's what I'm working toward:

- **Scheduled cron jobs** that automatically pull new data as it's generated
- **Code base** for automating intermediate data analysis—no more manual clicking
- **Clear, reproducible project structure** that anyone (including future you) can understand
- **Pre-defined statistics and visualization in dashboards** for at-a-glance insights
- **Accessible data** for multiple use cases:
  - Machine learning and LLM integration
  - Human intervention through visualization when needed
  - Rapid exploration using Shiny applications
  - Reproducible data management and exploration using Shinychat

A digitalized life science project in this vision looks like:

1. Data is exported to a common storage (e.g. AWS S3 bucket)
2. Scheduled jobs scan for new data and writes it using {pins}
3. The updated data triggers the {targets} pipeline, which re-runs the analysis
4. Results are written to Quarto dashboards, that gets published

### Set Up Automation:  
There are many Make for pipeline development: track changes, only run on upstream
changes

[![](https://docs.ropensci.org/targets/logo.svg){style="float: right;" width=150}](https://docs.ropensci.org/targets/)
The `{targets}` package will be the backbone of our reproducible, automated pipeline. 
It understands our analysis as a network of dependencies and automatically:
- Tracks what's changed since the last run
- Re-runs only the parts that need updating
- Parallelizes independent steps
- Provides a clear visual map of your entire workflow


### Develop relevant tooling for Intermediate Data Analysis
Seurat for analyzing single cell data.
**Your own tools:** This is where [`{flowplyr}`](https://hugoakerstrand.github.io/flowplyr/) comes in—my own contribution to making flow cytometry analysis more reproducible and pipeable. When existing tools don't quite fit your needs, the R package ecosystem makes it straightforward to build and share your own solutions.
<!-- [![](https://tidymodels.tidymodels.org/logo.png){style="float: right; margin: top 20px;" width=150}](https://tidymodels.tidymodels.org/) -->
<!-- [![](https://tidyverse.tidyverse.org/logo.png){style="float: right; margin: top 20px;" width=150}](https://tidyverse.tidyverse.org/) -->
<!-- [![](https://satijalab.org/seurat/output/images/SeuratV5.png){style="float: right; margin: top 20px;" width=150}](https://satijalab.org/seurat/https://satijalab.org/seurat/) -->

### Generate Dashboards: {Quarto}

[![](https://quarto.org/quarto.png){style="float: right; margin: top 20px;" width=150}](https://quarto.org)

Quarto is the next generation of R Markdown, and it's spectacular. Write your analysis in a mix of prose and code, then render it to:
- HTML dashboards for interactive exploration
- PDF reports for publication
- Word documents for collaborators who aren't ready for code
- Presentations for lab meetings
- Websites for sharing results

Your source code *is* your report. Change the data, re-render the document. Everything stays in sync, automatically.

### Other important packages for automation and digitalization
- Read, write, and version control data, objects, and models using [{pins}](https://pins.rstudio.com/) 
<!-- [![](https://pins.rstudio.com/logo.png){style="float: right; margin-top: 20px;" width=150}]() -->
- Create reproducible environment for a specific project using [{renv}](https://rstudio.github.io/renv/index.html)
<!-- [![](https://rstudio.github.io/renv/logo.svg){style="float: right; margin:top 20px;" width=150}](https://rstudio.github.io/renv/) -->


## Leverage genAI and LLM
[![](https://ellmer.tidyverse.org/logo.png){style="float: right; margin: top 20px;" width=150}](https://ellmer.tidyverse.org/)

[![](https://posit-dev.github.io/shinychat/r/logo.svg){style="float: right; margin: top 20px;" width=150}](https://posit-dev.github.io/shinychat/r/)

[![](https://github.com/rstudio/shiny/raw/main/man/figures/logo.png){style="float: right; margin: top 20px;" width=150}](https://shiny.posit.co/)

When dashboards aren't interactive enough, `{Shiny}` lets you build full web applications with no JavaScript required. But here's where it gets really exciting: `{shinychat}` brings conversational AI directly into your Shiny apps.

Imagine a dashboard where you can:
- Ask "show me all experiments from Q3 where the positive population exceeded 20%"
- Say "create a violin plot comparing these three conditions"
- Request "what's the statistical test I should use here?"
- Get help debugging your analysis in natural language

This is reproducible data management and exploration powered by LLMs. The future is arriving faster than most of us realize.
